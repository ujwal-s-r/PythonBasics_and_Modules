{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_81bimJLEWCC",
        "outputId": "d60ac80a-54c3-49da-d957-59b62e4c367a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying transformations...\n",
            "Plan created. No job has been executed yet.\n",
            "\n",
            "Calling an action to trigger the job...\n",
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "|Alice|\n",
            "|  Bob|\n",
            "|David|\n",
            "+-----+\n",
            "\n",
            "Job finished.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# 1. Create a SparkSession (the Driver connection)\n",
        "spark = SparkSession.builder.appName(\"LazyEvaluationExample\").getOrCreate()\n",
        "\n",
        "# 2. Create a DataFrame\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 28), (\"David\", 52)]\n",
        "columns = [\"name\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# 3. Apply Transformations (these are lazy and build the plan)\n",
        "print(\"Applying transformations...\")\n",
        "df_filtered = df.filter(df.age > 30)\n",
        "df_final = df_filtered.select(\"name\")\n",
        "\n",
        "# At this point, no actual data processing has happened.\n",
        "# Spark has just built an efficient plan (DAG) for the work.\n",
        "print(\"Plan created. No job has been executed yet.\")\n",
        "\n",
        "# 4. Call an Action (this triggers the execution)\n",
        "print(\"\\nCalling an action to trigger the job...\")\n",
        "df_final.show()\n",
        "\n",
        "# Now, the Driver sends the plan to the executors,\n",
        "# which read the data, filter it, select the column, and return the result.\n",
        "print(\"Job finished.\")\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code with RDD"
      ],
      "metadata": {
        "id": "-wKABJs6LlGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"RDDExample\").getOrCreate()\n",
        "\n",
        "# The SparkContext (sc) is the entry point for RDDs\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# --- Lesson 2.1: Creating and Manipulating RDDs ---\n",
        "print(\"--- Transformations ---\")\n",
        "# Create an RDD from a Python list\n",
        "numbers_rdd = sc.parallelize([1, 2, 3, 4, 5, 6])\n",
        "\n",
        "# Use map() to square each number\n",
        "squared_rdd = numbers_rdd.map(lambda x: x * x)\n",
        "print(\"Squared numbers (lazy):\", squared_rdd) # Note: this doesn't execute yet\n",
        "\n",
        "# Use filter() to get only the even numbers\n",
        "even_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# Create an RDD of sentences\n",
        "sentences_rdd = sc.parallelize([\"hello world\", \"spark is cool\"])\n",
        "\n",
        "# Use flatMap() to split sentences into words\n",
        "words_rdd = sentences_rdd.flatMap(lambda sentence: sentence.split(\" \"))\n",
        "\n",
        "\n",
        "# --- Lesson 2.2: Aggregations and Actions ---\n",
        "print(\"\\n--- Aggregations and Actions ---\")\n",
        "# Create a key-value pair RDD for word counting\n",
        "word_pairs_rdd = words_rdd.map(lambda word: (word, 1))\n",
        "\n",
        "# Use reduceByKey() to count the occurrences of each word\n",
        "word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Call an ACTION to trigger all the above transformations and see the result\n",
        "print(\"\\nWord Counts:\")\n",
        "word_counts_result = word_counts_rdd.collect() # DANGEROUS on large data\n",
        "for word, count in word_counts_result:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "print(\"\\nFirst 3 squared numbers:\")\n",
        "# Use take() to safely get a few results\n",
        "first_three_squared = squared_rdd.take(3)\n",
        "print(first_three_squared)\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QvQB_RsLkLa",
        "outputId": "82c5f47c-3411-4bfa-f86a-7694d2af2bc9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Transformations ---\n",
            "Squared numbers (lazy): PythonRDD[1] at RDD at PythonRDD.scala:53\n",
            "\n",
            "--- Aggregations and Actions ---\n",
            "\n",
            "Word Counts:\n",
            "hello: 1\n",
            "world: 1\n",
            "spark: 1\n",
            "is: 1\n",
            "cool: 1\n",
            "\n",
            "First 3 squared numbers:\n",
            "[1, 4, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ”¹ Reduce\n",
        "\n",
        "Runs a function to combine multiple values into one.\n",
        "\n",
        "For each key group in reduceByKey, Spark repeatedly applies your function to pairs of values until only one remains."
      ],
      "metadata": {
        "id": "WVm5AO5FPTEb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Itâ€™s not just a normal \"apply\". Itâ€™s Spark doing reduce: take two values, combine them, repeat, until one value is left for that key."
      ],
      "metadata": {
        "id": "au4HTUKIPZIc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qix0B7ogPZkv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}